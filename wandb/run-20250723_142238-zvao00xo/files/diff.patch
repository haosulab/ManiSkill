diff --git a/mani_skill/envs/tasks/tabletop/pick_cube.py b/mani_skill/envs/tasks/tabletop/pick_cube.py
index 40fb72c..0b6352e 100644
--- a/mani_skill/envs/tasks/tabletop/pick_cube.py
+++ b/mani_skill/envs/tasks/tabletop/pick_cube.py
@@ -149,6 +149,13 @@ class PickCubeEnv(BaseEnv):
                 obj_to_goal_pos=self.goal_site.pose.p - self.cube.pose.p,
             )
         return obs
+    
+    def _strip_finger_vel(self, qvel):
+        if self.robot_uids in ["panda", "widowxai", "xarm6_robotiq"]:
+            return qvel[..., :-2]       
+        elif self.robot_uids == "so100":
+            return qvel[..., :-1]       
+        return qvel
 
     def evaluate(self):
         is_obj_placed = (
@@ -156,7 +163,11 @@ class PickCubeEnv(BaseEnv):
             <= self.goal_thresh
         )
         is_grasped = self.agent.is_grasping(self.cube)
-        is_robot_static = self.agent.is_static(0.2)
+
+        qvel = self._strip_finger_vel(self.agent.robot.get_qvel())
+        static_thresh = 0.5 if self.robot_uids.startswith("xarm6") else 0.2
+        is_robot_static = torch.linalg.norm(qvel, dim=1) < static_thresh
+
         return {
             "success": is_obj_placed & is_robot_static,
             "is_obj_placed": is_obj_placed,
@@ -168,22 +179,20 @@ class PickCubeEnv(BaseEnv):
         tcp_to_obj_dist = torch.linalg.norm(
             self.cube.pose.p - self.agent.tcp.pose.p, axis=1
         )
-        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)
+        reaching = 1 - torch.tanh(5 * tcp_to_obj_dist)
 
         is_grasped = info["is_grasped"]
 
         obj_to_goal_dist = torch.linalg.norm(
             self.goal_site.pose.p - self.cube.pose.p, axis=1
         )
-        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)
-        place_reward *= is_grasped
+        placing = (1 - torch.tanh(5 * obj_to_goal_dist)) * is_grasped
 
-        static_reward = 1 - torch.tanh(
-            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)
-        )
-        static_reward *= info["is_obj_placed"]
+        qvel = self._strip_finger_vel(self.agent.robot.get_qvel())
+        static = 1 - torch.tanh(5 * torch.linalg.norm(qvel, axis=1))
+        static *= info["is_obj_placed"]
 
-        return reaching_reward.mean(), is_grasped.mean(), place_reward.mean(), static_reward.mean()
+        return reaching.mean(), is_grasped.mean(), placing.mean(), static.mean()
 
     def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
         tcp_to_obj_dist = torch.linalg.norm(
@@ -202,7 +211,7 @@ class PickCubeEnv(BaseEnv):
         reward += place_reward * is_grasped
 
         qvel = self.agent.robot.get_qvel()
-        if self.robot_uids in ["panda", "widowxai"]:
+        if self.robot_uids in ["panda", "widowxai", "xarm6_robotiq"]:
             qvel = qvel[..., :-2]
         elif self.robot_uids == "so100":
             qvel = qvel[..., :-1]
