diff --git a/mani_skill/envs/tasks/tabletop/pick_cube.py b/mani_skill/envs/tasks/tabletop/pick_cube.py
index 40fb72c..1dff21e 100644
--- a/mani_skill/envs/tasks/tabletop/pick_cube.py
+++ b/mani_skill/envs/tasks/tabletop/pick_cube.py
@@ -48,7 +48,7 @@ class PickCubeEnv(BaseEnv):
     ]
     agent: Union[Panda, Fetch, XArm6Robotiq, SO100, WidowXAI]
     cube_half_size = 0.02
-    goal_thresh = 0.025
+    goal_thresh = 0.1
     cube_spawn_half_size = 0.05
     cube_spawn_center = (0, 0)
 
@@ -149,6 +149,23 @@ class PickCubeEnv(BaseEnv):
                 obj_to_goal_pos=self.goal_site.pose.p - self.cube.pose.p,
             )
         return obs
+    
+    def _strip_finger_vel(self, qvel):
+        if self.robot_uids in ["panda", "widowxai", "xarm6_robotiq"]:
+            return qvel[..., :-2]       
+        elif self.robot_uids == "so100":
+            return qvel[..., :-1]       
+        return qvel
+    
+    def _debug_static_check(self, tag=""):
+        qvel_full = self.agent.robot.get_qvel()
+        qvel_strip = self._strip_finger_vel(qvel_full)
+
+        print(f"{tag}"
+            f"  vel_full={torch.linalg.norm(qvel_full,  dim=1)[0]:.3f}"
+            f"  vel_strp={torch.linalg.norm(qvel_strip, dim=1)[0]:.3f}"
+            f"  obj_goal={torch.linalg.norm(self.goal_site.pose.p - self.cube.pose.p, dim=1)[0]:.3f}")
+
 
     def evaluate(self):
         is_obj_placed = (
@@ -156,7 +173,14 @@ class PickCubeEnv(BaseEnv):
             <= self.goal_thresh
         )
         is_grasped = self.agent.is_grasping(self.cube)
-        is_robot_static = self.agent.is_static(0.2)
+
+        qvel = self._strip_finger_vel(self.agent.robot.get_qvel())
+        static_thresh = 1.0 if self.robot_uids.startswith("xarm6") else 0.2
+        is_robot_static = torch.linalg.norm(qvel, dim=1) < static_thresh
+        
+        # DEBUG
+        # self._debug_static_check(tag="eval")
+
         return {
             "success": is_obj_placed & is_robot_static,
             "is_obj_placed": is_obj_placed,
@@ -168,22 +192,20 @@ class PickCubeEnv(BaseEnv):
         tcp_to_obj_dist = torch.linalg.norm(
             self.cube.pose.p - self.agent.tcp.pose.p, axis=1
         )
-        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)
+        reaching = 1 - torch.tanh(5 * tcp_to_obj_dist)
 
         is_grasped = info["is_grasped"]
 
         obj_to_goal_dist = torch.linalg.norm(
             self.goal_site.pose.p - self.cube.pose.p, axis=1
         )
-        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)
-        place_reward *= is_grasped
+        placing = (1 - torch.tanh(5 * obj_to_goal_dist)) * is_grasped
 
-        static_reward = 1 - torch.tanh(
-            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)
-        )
-        static_reward *= info["is_obj_placed"]
+        qvel = self._strip_finger_vel(self.agent.robot.get_qvel())
+        static = 1 - torch.tanh(5 * torch.linalg.norm(qvel, axis=1))
+        static *= info["is_obj_placed"]
 
-        return reaching_reward.mean(), is_grasped.mean(), place_reward.mean(), static_reward.mean()
+        return reaching.mean(), is_grasped.mean(), placing.mean(), static.mean()
 
     def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
         tcp_to_obj_dist = torch.linalg.norm(
@@ -202,7 +224,7 @@ class PickCubeEnv(BaseEnv):
         reward += place_reward * is_grasped
 
         qvel = self.agent.robot.get_qvel()
-        if self.robot_uids in ["panda", "widowxai"]:
+        if self.robot_uids in ["panda", "widowxai", "xarm6_robotiq"]:
             qvel = qvel[..., :-2]
         elif self.robot_uids == "so100":
             qvel = qvel[..., :-1]
