# Datasets

ManiSkill has a wide variety of demonstrations from different sources including RL, human teleoperation, and motion-planning.

## Download

We provide a command line tool to download demonstrations directly from our [Hugging Face ðŸ¤— dataset page](https://huggingface.co/datasets/haosulab/ManiSkill2) which are done by task ID. The tool will download the demonstration files to a folder and also a few demonstration videos visualizing what the demonstrations look like. See [Tasks](../concepts/environments.md) for a list of all supported tasks.

<!-- TODO: add a table here detailing the data info in detail -->
<!-- Please see our [notes](https://docs.google.com/document/d/1bBKmsR-R_7tR9LwaT1c3J26SjIWw27tWSLdHnfBR01c/edit?usp=sharing) about the details of the demonstrations. -->

```bash
# Download the full datasets
python -m mani_skill.utils.download_demo all
# Download the demonstration dataset for certain task
python -m mani_skill.utils.download_demo ${ENV_ID}
# Download the demonstration datasets for all rigid-body tasks to "./demos"
python -m mani_skill.utils.download_demo rigid_body -o ./demos
```

## Format

All demonstrations for an task are saved in the HDF5 format openable by [h5py](https://github.com/h5py/h5py). Each HDF5 dataset is named `trajectory.{obs_mode}.{control_mode}.h5`, and is associated with a JSON metadata file with the same base name. Unless otherwise specified, `trajectory.h5` is short for `trajectory.none.pd_joint_pos.h5`, which contains the original demonstrations generated by the `pd_joint_pos` controller with the `none` observation mode (empty observations). However, there may exist demonstrations generated by other controllers. **Thus, please check the associated JSON to ensure which controller is used.**
<!-- 
:::{note}
For `PickSingleYCB-v0`, `TurnFaucet-v0`, the dataset is named `{model_id}.h5` for each asset. It is due to some legacy issues, and might be changed in the future.

For `OpenCabinetDoor-v1`, `OpenCabinetDrawer-v1`, `PushChair-v1`, `MoveBucket-v1`, which are migrated from [ManiSkill1](https://github.com/haosulab/ManiSkill), trajectories are generated by the RL and `base_pd_joint_vel_arm_pd_joint_vel` controller.
::: -->

### Meta Information (JSON)

Each JSON file contains:

- `env_info` (Dict): task (also known as environment) information, which can be used to initialize the task
  - `env_id` (str): task id
  - `max_episode_steps` (int)
  - `env_kwargs` (Dict): keyword arguments to initialize the task. **Essential to recreate the environment.**
- `episodes` (List[Dict]): episode information
- `source_type` (Optional[str]): a simple category string describing what process generated the trajectory data. ManiSkill official datasets will usually write one of "human", "motionplanning", or "rl" at the moment.
- `source_desc` (Optional[str]): a longer explanation of how the data was generated.

The episode information (the element of `episodes`) includes:

- `episode_id` (int): a unique id to index the episode
- `reset_kwargs` (Dict): keyword arguments to reset the task. **Essential to reproduce the trajectory.**
- `control_mode` (str): control mode used for the episode.
- `elapsed_steps` (int): trajectory length
- `info` (Dict): information at the end of the episode.

With just the meta data, you can reproduce the task the same way it was created when the trajectories were collected as so:

```python
env = gym.make(env_info["env_id"], **env_info["env_kwargs"])
episode = env_info["episodes"][0] # picks the first
env.reset(**episode["reset_kwargs"])
```

### Trajectory Data (HDF5)

Each HDF5 demonstration dataset consists of multiple trajectories. The key of each trajectory is `traj_{episode_id}`, e.g., `traj_0`.

Each trajectory is an `h5py.Group`, which contains:

- actions: [T, A], `np.float32`. `T` is the number of transitions.
- terminated: [T], `np.bool_`. It indicates whether the task is terminated or not at each time step.
- truncated: [T], `np.bool_`. It indicates whether the task is truncated or not at each time step.
- env_states: [T+1, D], `np.float32`. Environment states. It can be used to set the environment to a certain state via `env.set_state_dict`. However, it may not be enough to reproduce the trajectory.
- success (optional): [T], `np.bool_`. It indicates whether the task is successful at each time step. Included if task defines success.
- fail (optional): [T], `np.bool_`. It indicates whether the task is in a failure state at each time step. Included if task defines failure.
- obs (optional): [T+1, D] observations.

Note that env_states is in a dictionary form (and observations may be as well depending on obs_mode), where it is formatted as a dictionary of lists. For example, a typical environment state looks like this:

```python
env_state = env.get_state_dict()
"""
env_state = {
  "actors": {
    "actor_id": [...numpy_actor_state...],
    ...
  },
  "articulations": {
    "articulation_id": [...numpy_articulation_state...],
    ...
  }
}
"""
```
In the trajectory file env_states will be the same structure but each value/leaf in the dictionary will be a sequence of states representing the state of that particular entity in the simulation over time.

In practice it is may be more useful to use slices of the env_states data (or the observations data), which can be done with
```python
import mani_skill.trajectory.utils as trajectory_utils
env_states = trajectory_utils.dict_to_list_of_dicts(env_states)
# now env_states[i] is the same as the data env.get_state_dict() returned at timestep i
i = 10
env_state_i = trajectory_utils.index_dict(env_states, i)
# now env_state_i is the same as the data env.get_state_dict() returned at timestep i
```

These tools are also used in the PyTorch Dataset implementation we provide which is explained the nect section

## Loading Trajectory Datasets

#### PyTorch

We provide an example way to build a PyTorch Dataset and easily load the trajectory .h5 data at https://github.com/haosulab/ManiSkill2/tree/dev/mani_skill/trajectory/datasets.py.


<!-- #### Other -->
