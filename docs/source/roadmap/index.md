# Roadmap

We are constantly working to improve ManiSkill on all aspects, from simulation/rendering speed to usability. Below is a brief list of planned upcoming features that will come to ManiSkill in the future


- GPU parallelized low-level mobile manipulation tasks (at the moment you can just take random actions but there are no reward/success metrics)
- More fully interactive scene datasets
- More industrial robots properly modelled and added into simulation (Google Robot, UR5, Unitree H1 etc.)
- Larger demonstration datasets, especially tele-operated demonstrations
- Offline RL Baselines (Algorithms like IQL, CQL, etc.)
- Behavior Cloning Baselines (Algorithms like 3D Diffusion Policy, etc.)
- Vision Language Model Baselines (Algorithms like PerAct, VoxPoser, etc.)
- Easy to use domain randomization tools (randomizing lighting, camera poses, object textures etc.)
- Wider variety of verified tasks
- A new, fast and accurate soft body simulator
- Simple beautification tools to make high quality renders/recordings of trajectories in simulation
- Object database of high quality, simlation ready assets (rigid-body, soft-body, articulations) for use in your tasks
- More sensor types like Lidar
- Real2Sim evaluation pipelines